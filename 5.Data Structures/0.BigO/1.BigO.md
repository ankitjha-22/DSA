# Big O notation

Big O notation is a mathematical way to describe the efficiency of algorithms, particularly how they scale with the size of the input data. It's a measure of an algorithm's performance in terms of both time and space as input sizes grow, focusing on the "worst-case scenario" to set an upper bound on the running time or memory usage.

# Big O Notation: Key Points to Understand Big O Notation

## Focus on Growth Rate

- **Big O** describes how an algorithm's time or space requirements grow as the input size increases.
- It focuses on the **dominant term** of the growth rate, ignoring constants and less significant terms because they have minimal impact as inputs grow large.

## Types of Complexity

Big O notation is primarily used to describe two types of complexity:

1. **Time Complexity**: Measures the time taken by an algorithm relative to input size.
2. **Space Complexity**: Measures the memory required by an algorithm as input size increases.

## How to Read Big O Notation

- Big O is written as \( O(f(n)) \), where \( f(n) \) represents a function of the input size \( n \).
- **Example**: \( O(n^2) \) means the time or space requirements increase **quadratically** with the input size.

## Common Complexity Classes

- **O(1)**: Constant time — the algorithm takes a fixed amount of time regardless of input size.
- **O(log n)**: Logarithmic time — the algorithm's time grows logarithmically as input size increases.
- **O(n)**: Linear time — the algorithm's time grows proportionally with the input size.
- **O(n log n)**: Linearithmic time — a combination of linear and logarithmic growth.
- **O(n^2)**: Quadratic time — the algorithm's time grows proportionally to the square of the input size.
- **O(2^n)**: Exponential time — the algorithm's time doubles with each additional input element.
- **O(n!)**: Factorial time — the algorithm's time grows factorially, which is very slow even for small inputs.

These Big O complexities help understand how an algorithm performs as the input size scales, allowing for comparison and optimization.

## Examples of Big O in Code

### 1. O(1)

```
function getFirstElement(arr) {
    return arr[0];
}

```

### 2. O(n)

```
function sumArray(arr) {
    let sum = 0;
    for (let i = 0; i < arr.length; i++) {
        sum += arr[i];
    }
    return sum;
}


```

### 3. O(n^2)

```
function printAllPairs(arr) {
    for (let i = 0; i < arr.length; i++) {
        for (let j = 0; j < arr.length; j++) {
            console.log(arr[i], arr[j]);
        }
    }
}


```

### 4. O(log n)

```
function binarySearch(arr, target) {
    let left = 0;
    let right = arr.length - 1;
    while (left <= right) {
        let mid = Math.floor((left + right) / 2);
        if (arr[mid] === target) return mid;
        else if (arr[mid] < target) left = mid + 1;
        else right = mid - 1;
    }
    return -1;
}

```

# Simplifying Big O Expressions: Key Rules

When simplifying Big O expressions, these general rules help reduce the complexity notation to its most significant term:

### 1. Remove Constants

Constants do not scale with input size, so they can be dropped.

- **Example**: \( O(3n) \) simplifies to \( O(n) \).

### 2. Drop Non-Dominant Terms

Only keep the term with the largest growth rate as \( n \) increases. Lower-order terms have minimal impact as \( n \) grows large.

- **Example**: \( O(n^2 + n) \) simplifies to \( O(n^2) \), since \( n^2 \) grows faster than \( n \) as \( n \) increases.

### 3. Multiplication Rule (for Nested Loops or Function Calls)

When an algorithm has nested operations, their complexities multiply.

- **Example**: If a function makes \( O(\log n) \) recursive calls, and each call performs \( O(n) \) work, the overall complexity is \( O(n \cdot \log n) \).

### 4. Addition Rule (for Sequential Operations)

When two or more independent operations occur sequentially, their complexities are added.

- **Example**: If an algorithm does an \( O(n^2) \) task followed by an \( O(n) \) task, the total complexity is \( O(n^2 + n) \), which simplifies to \( O(n^2) \) by dropping the non-dominant term.

### 5. Exponential Dominance Rule

Exponential terms grow faster than polynomial and logarithmic terms, so they are always the dominant terms.

- **Example**: \( O(2^n + n^2) \) simplifies to \( O(2^n) \) because exponential growth outpaces polynomial growth.

### 6. Logarithmic Simplification

Logs of different bases are equivalent under Big O, so \( O(\log*2 n) \), \( O(\log*{10} n) \), and \( O(\ln n) \) all simplify to \( O(\log n) \).

- **Example**: \( O(\log*2 n + \log*{10} n) \) simplifies to \( O(\log n) \).

### 7. Polynomial Dominance

Higher-degree polynomials dominate lower-degree polynomials.

- **Example**: \( O(n^3 + n^2) \) simplifies to \( O(n^3) \) because \( n^3 \) grows faster than \( n^2 \).

### 8. Combine Terms with Similar Growth Rates

Terms that grow at the same rate can be combined.

- **Example**: \( O(2n + 3n) \) simplifies to \( O(5n) \), which further simplifies to \( O(n) \) by removing the constant.

### 9. Factorials are Dominant over Exponentials

Factorial growth rates are faster than exponential growth, so any polynomial or exponential term can be dropped in favor of a factorial term.

- **Example**: \( O(n! + 2^n) \) simplifies to \( O(n!) \).

---

| Rule                           | Example                | Simplified Form  |
| ------------------------------ | ---------------------- | ---------------- |
| **Remove Constants**           | \( O(5n) \)            | \( O(n) \)       |
| **Drop Non-Dominant Terms**    | \( O(n^2 + n) \)       | \( O(n^2) \)     |
| **Multiplication (Nested)**    | \( O(n) \* O(log n) \) | \( O(n log n) \) |
| **Addition (Sequential)**      | \( O(n^2) + O(n) \)    | \( O(n^2) \)     |
| **Exponential Dominance**      | \( O(2^n + n^3) \)     | \( O(2^n) \)     |
| **Logarithmic Simplification** | \( O(log\_{10} n) \)   | \( O(log n) \)   |
| **Polynomial Dominance**       | \( O(n^3 + n^2) \)     | \( O(n^3) \)     |
| **Combine Similar Rates**      | \( O(2n + 3n) \)       | \( O(n) \)       |
| **Factorial Dominance**        | \( O(n! + 2^n) \)      | \( O(n!) \)      |
